{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 43.61%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# Step 1: Decision Tree Class\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth, min_size):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_size = min_size\n",
    "        self.tree = None\n",
    "\n",
    "    # Step 2: Load the dataset from CSV\n",
    "    def load_data(self, filepath):\n",
    "        data = pd.read_csv(filepath)\n",
    "        dataset = data.values.tolist()  # Convert to list of lists\n",
    "        return dataset\n",
    "\n",
    "    # Step 3: Gini Impurity calculation\n",
    "    def gini_impurity(self, groups, classes):\n",
    "        n_instances = sum([len(group) for group in groups])  # Total instances\n",
    "        gini = 0.0\n",
    "        for group in groups:\n",
    "            size = len(group)\n",
    "            if size == 0:\n",
    "                continue\n",
    "            score = 0.0\n",
    "            for class_val in classes:\n",
    "                proportion = (group.count(class_val) / size)\n",
    "                score += proportion ** 2\n",
    "            gini += (1.0 - score) * (size / n_instances)\n",
    "        return gini\n",
    "\n",
    "    # Step 4: Split the dataset\n",
    "    def test_split(self, index, value, dataset):\n",
    "        left, right = [], []\n",
    "        for row in dataset:\n",
    "            if row[index] < value:\n",
    "                left.append(row)\n",
    "            else:\n",
    "                right.append(row)\n",
    "        return left, right\n",
    "\n",
    "    # Step 5: Find the best split\n",
    "    def get_split(self, dataset):\n",
    "        class_values = list(set(row[-1] for row in dataset))\n",
    "        b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "        for index in range(len(dataset[0]) - 1):  # Exclude target column\n",
    "            if index == 0:  # Skip the 'questiontype' feature (assuming it's the first feature)\n",
    "                continue\n",
    "            for row in dataset:\n",
    "                groups = self.test_split(index, row[index], dataset)\n",
    "                gini = self.gini_impurity(groups, class_values)\n",
    "                if gini < b_score:\n",
    "                    b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
    "        return {'index': b_index, 'value': b_value, 'groups': b_groups}\n",
    "\n",
    "    # Step 6: Create terminal node (leaf node)\n",
    "    def to_terminal(self, group):\n",
    "        outcomes = [row[-1] for row in group]\n",
    "        return max(set(outcomes), key=outcomes.count)\n",
    "\n",
    "    # Step 7: Split a node recursively\n",
    "    def split(self, node, depth):\n",
    "        left, right = node['groups']\n",
    "        del(node['groups'])\n",
    "        if not left or not right:\n",
    "            node['left'] = node['right'] = self.to_terminal(left + right)\n",
    "            return\n",
    "        if depth >= self.max_depth:\n",
    "            node['left'], node['right'] = self.to_terminal(left), self.to_terminal(right)\n",
    "            return\n",
    "        if len(left) <= self.min_size:\n",
    "            node['left'] = self.to_terminal(left)\n",
    "        else:\n",
    "            node['left'] = self.get_split(left)\n",
    "            self.split(node['left'], depth + 1)\n",
    "\n",
    "        if len(right) <= self.min_size:\n",
    "            node['right'] = self.to_terminal(right)\n",
    "        else:\n",
    "            node['right'] = self.get_split(right)\n",
    "            self.split(node['right'], depth + 1)\n",
    "\n",
    "    # Step 8: Build the decision tree\n",
    "    def build_tree(self, train):\n",
    "        root = self.get_split(train)\n",
    "        self.split(root, 1)\n",
    "        return root\n",
    "\n",
    "    # Step 9: Make prediction\n",
    "    def predict(self, node, row):\n",
    "        if row[node['index']] < node['value']:\n",
    "            if isinstance(node['left'], dict):\n",
    "                return self.predict(node['left'], row)\n",
    "            else:\n",
    "                return node['left']\n",
    "        else:\n",
    "            if isinstance(node['right'], dict):\n",
    "                return self.predict(node['right'], row)\n",
    "            else:\n",
    "                return node['right']\n",
    "\n",
    "    # Step 10: Predict for all rows in the test dataset\n",
    "    def predict_all(self, tree, test):\n",
    "        predictions = []\n",
    "        for row in test:\n",
    "            prediction = self.predict(tree, row)\n",
    "            predictions.append(prediction)\n",
    "        return predictions\n",
    "\n",
    "    # Step 11: Accuracy Metric\n",
    "    def accuracy(self, actual, predicted):\n",
    "        correct = sum([1 if actual[i] == predicted[i] else 0 for i in range(len(actual))])\n",
    "        return correct / len(actual) * 100.0\n",
    "\n",
    "    # Step 12: Split dataset into training and testing\n",
    "    def train_test_split(self, dataset, split_ratio=0.7):\n",
    "        train_size = int(len(dataset) * split_ratio)\n",
    "        train_set = dataset[:train_size]\n",
    "        test_set = dataset[train_size:]\n",
    "        return train_set, test_set\n",
    "\n",
    "    # Step 13: Train and evaluate the model\n",
    "    def train_and_evaluate(self, filepath):\n",
    "        dataset = self.load_data(filepath)\n",
    "\n",
    "        # Split into train and test datasets\n",
    "        train, test = self.train_test_split(dataset)\n",
    "\n",
    "        # Build the decision tree\n",
    "        self.tree = self.build_tree(train)\n",
    "\n",
    "        # Evaluate the model\n",
    "        actual = [row[-1] for row in test]\n",
    "        predicted = self.predict_all(self.tree, test)\n",
    "        accuracy = self.accuracy(actual, predicted)\n",
    "        print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "        return self.tree\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the decision tree model\n",
    "    max_depth = 5\n",
    "    min_size = 10\n",
    "    model = DecisionTree(max_depth=max_depth, min_size=min_size)\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    tree = model.train_and_evaluate(\"training data/preprocessed_trainingdataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index': 1,\n",
      " 'left': {'index': 1, 'left': 1.0, 'right': 1.0, 'value': 0.0},\n",
      " 'right': {'index': 1, 'left': 1.0, 'right': 1.0, 'value': 1.0},\n",
      " 'value': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(tree)  # Display the full tree structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the trained decision tree\n",
    "with open('custom_decision_tree_model.pkl', 'wb') as file:\n",
    "    pickle.dump(tree, file)\n",
    "\n",
    "print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the trained decision tree model\n",
    "with open('custom_decision_tree_model.pkl', 'rb') as file:\n",
    "    loaded_tree = pickle.load(file)\n",
    "\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: <class 'dict'>\n",
      "The model is a dictionary (tree structure).\n",
      "\n",
      "Model structure (root node):\n",
      "{'index': 1, 'value': 1.0, 'left': {'index': 1, 'value': 0.0, 'left': 1.0, 'right': 1.0}, 'right': {'index': 1, 'value': 1.0, 'left': 1.0, 'right': 1.0}}\n",
      "\n",
      "Prediction for the sample row: 1.0\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the model from the pickle file\n",
    "with open('custom_decision_tree_model.pkl', 'rb') as model_file:\n",
    "    model = pickle.load(model_file)\n",
    "\n",
    "# 1. Check the type of the model\n",
    "print(\"Model type:\", type(model))\n",
    "\n",
    "# 2. Check if it's a dictionary (since you are saving a custom decision tree in dictionary format)\n",
    "if isinstance(model, dict):\n",
    "    print(\"The model is a dictionary (tree structure).\")\n",
    "else:\n",
    "    print(\"The model is not a dictionary. Something might be wrong.\")\n",
    "\n",
    "# 3. Check the model's structure by printing the root node\n",
    "print(\"\\nModel structure (root node):\")\n",
    "print(model)\n",
    "\n",
    "# 4. Test the model on a small known dataset (e.g., a sample row) for prediction\n",
    "sample_row = [-1200,200,111,24,56,56889,-100000,-20000]  # Replace this with a valid feature set based on your dataset\n",
    "print(\"\\nPrediction for the sample row:\", predict(model, sample_row))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from rake_nltk import Rake\n",
    "import sqlite3\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# SQLite database connection\n",
    "conn = sqlite3.connect('questions.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Helper functions (same as before)\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word not in string.punctuation and word not in stop_words]\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    stemmed = [stemmer.stem(word) for word in lemmatized]\n",
    "    return stemmed\n",
    "\n",
    "def extract_qtype(question):\n",
    "    rake.extract_keywords_from_text(question)\n",
    "    keywords = rake.get_ranked_phrases()\n",
    "    computational_keywords = {\"perform\", \"generate\", \"database\", \"create\", \"design\", \"implement\", \"calculate\", \"determine\", \"solve\", \"compute\", \"evaluate\", \"simplify\"}\n",
    "    descriptive_keywords = {\"justify\", \"method\", \"process\", \"procedure\", \"solution\", \"explain\", \"describe\", \"illustrate\", \"state\", \"discuss\", \"write\", \"why\"}\n",
    "    proof_keywords = {\"prove\", \"verify\", \"demonstrate\", \"show\"}\n",
    "    statistical_keywords = {\"test\", \"hypothesis\", \"mean\", \"variance\", \"confidence\", \"significance\"}\n",
    "    what_keywords = {\"what\", \"define\", \"list\"}\n",
    "    differentiate_keywords = {\"differentiate\", \"contrast\", \"compare\"}\n",
    "    mathematical_keywords = {\"solve\", \"convert\", \"find\", \"equation\", \"function\", \"integral\", \"derivative\", \"matrix\"}\n",
    "    discrete_keywords = {\"graph\", \"logic\", \"combinatorics\", \"relation\", \"algorithm\"}\n",
    "    \n",
    "    scores = {\"Computational\": 0, \"Descriptive\": 0, \"Proof\": 0, \"Statistical\": 0, \"What\": 0, \"Differentiate\": 0, \"Mathematical\": 0, \"Discrete\": 0, \"Other\": 0}\n",
    "\n",
    "    for phrase in keywords:\n",
    "        words = phrase.split()\n",
    "        for word in words:\n",
    "            if word in computational_keywords:\n",
    "                scores[\"Computational\"] += 1\n",
    "            elif word in descriptive_keywords:\n",
    "                scores[\"Descriptive\"] += 1\n",
    "            elif word in proof_keywords:\n",
    "                scores[\"Proof\"] += 1\n",
    "            elif word in statistical_keywords:\n",
    "                scores[\"Statistical\"] += 1\n",
    "            elif word in what_keywords:\n",
    "                scores[\"What\"] += 1\n",
    "            elif word in differentiate_keywords:\n",
    "                scores[\"Differentiate\"] += 1\n",
    "            elif word in mathematical_keywords:\n",
    "                scores[\"Mathematical\"] += 1\n",
    "            elif word in discrete_keywords:\n",
    "                scores[\"Discrete\"] += 1\n",
    "\n",
    "    max_score_type = max(scores, key=scores.get)\n",
    "    return \"Other\" if scores[max_score_type] == 0 else max_score_type\n",
    "\n",
    "def keyword_count(question):\n",
    "    rake.extract_keywords_from_text(question)\n",
    "    keywords = rake.get_ranked_phrases()\n",
    "    return len(keywords)\n",
    "\n",
    "def compute_tf_idf(questions):\n",
    "    tf = []\n",
    "    idf = {}\n",
    "    N = len(questions)\n",
    "    vocabulary = set()\n",
    "    processed_questions = [preprocess_text(q) for q in questions]\n",
    "    for q in processed_questions:\n",
    "        freq = {}\n",
    "        for word in q:\n",
    "            vocabulary.add(word)\n",
    "            freq[word] = freq.get(word, 0) + 1\n",
    "        tf.append(freq)\n",
    "\n",
    "    for word in vocabulary:\n",
    "        count = sum(1 for q in processed_questions if word in q)\n",
    "        idf[word] = np.log(N / (1 + count))\n",
    "\n",
    "    tf_idf_scores = []\n",
    "    for q_tf in tf:\n",
    "        scores = {word: tf_val * idf[word] for word, tf_val in q_tf.items()}\n",
    "        tf_idf_scores.append(scores)\n",
    "    return tf_idf_scores\n",
    "\n",
    "def avg_word_length(text):\n",
    "    question_tokens = preprocess_text(text)\n",
    "    total_length = sum(len(word) for word in question_tokens)\n",
    "    return 0 if not question_tokens else total_length / len(question_tokens)\n",
    "\n",
    "def sentence_count(question):\n",
    "    return len(re.split(r'[.!?]', question)) - 1\n",
    "\n",
    "def readability_score(question):\n",
    "    sentences = sent_tokenize(question)\n",
    "    words = word_tokenize(question)\n",
    "    complex_words = [word for word in words if len(word) > 2 and word not in string.punctuation]\n",
    "    word_count = len(words)\n",
    "    sentence_count = len(sentences)\n",
    "    complex_word_count = len(complex_words)\n",
    "    \n",
    "    if sentence_count == 0 or word_count == 0:\n",
    "        return 0\n",
    "    gunning_fog = 0.4 * ((word_count / sentence_count) + 100 * (complex_word_count / word_count))\n",
    "    return gunning_fog\n",
    "\n",
    "# Initialize lemmatizer, stemmer, and stop words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# RAKE initialization\n",
    "rake = Rake(stopwords=stop_words)\n",
    "\n",
    "# Retrieve data from SQLite database\n",
    "cursor.execute(\"SELECT question FROM questionlist\")\n",
    "questions = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "# Extract features for each question and store them in a list\n",
    "features_data = []\n",
    "\n",
    "for question in questions:\n",
    "    features = []\n",
    "    features.append(extract_qtype(question))  # qtype\n",
    "    features.append(keyword_count(question))  # keyword count\n",
    "    tf_idf_scores = compute_tf_idf([question])\n",
    "    features.append(np.mean(list(tf_idf_scores[0].values())) if tf_idf_scores[0] else 0)  # TF-IDF score\n",
    "    features.append(sentence_count(question))  # sentence count\n",
    "    features.append(readability_score(question))  # readability score\n",
    "    features.append(avg_word_length(question))  # average word length\n",
    "    features_data.append(features)\n",
    "\n",
    "# Convert the feature list into a DataFrame\n",
    "features_df = pd.DataFrame(features_data, columns=[\"qtype\", \"keyword_count\", \"keyword_tfidf\", \"sentence_count\", \"readability_scores\", \"avg_word_length\"])\n",
    "\n",
    "# One-hot encoding for qtype\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "qtype_encoded = encoder.fit_transform(features_df[[\"qtype\"]])\n",
    "qtype_encoded_df = pd.DataFrame(qtype_encoded, columns=encoder.get_feature_names_out([\"qtype\"]))\n",
    "features_df = pd.concat([features_df.reset_index(drop=True), qtype_encoded_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Select numeric features only\n",
    "features_df_numeric = features_df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Fill missing values in the numeric feature columns using the mean\n",
    "features_df_numeric = features_df_numeric.fillna(features_df_numeric.mean())\n",
    "\n",
    "# Load the pre-trained custom decision tree model from the .pkl file\n",
    "with open('custom_decision_tree_model.pkl', 'rb') as model_file:\n",
    "    model = pickle.load(model_file)\n",
    "\n",
    "# Function to predict using the custom decision tree\n",
    "def predict(node, row):\n",
    "    if row[node['index']] < node['value']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            return predict(node['left'], row)\n",
    "        else:\n",
    "            return node['left']\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return predict(node['right'], row)\n",
    "        else:\n",
    "            return node['right']\n",
    "\n",
    "# Make predictions using the pre-trained custom decision tree model\n",
    "predictions = [predict(model, row) for row in features_df_numeric.values]\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index': 1, 'value': 1.0, 'left': {'index': 1, 'value': 0.0, 'left': 1.0, 'right': 1.0}, 'right': {'index': 1, 'value': 1.0, 'left': 1.0, 'right': 1.0}}\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
